{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "# Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4RCyyPSYRp"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGCtiXUhSWss"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "vocab_size = 10000 #vocab size\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCPC_WN-eCyw"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "vocab_size = 10000 #vocab size\n",
    "maxlen = 20  #number of word used from each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMEsHYrWxdtk"
   },
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0g381XzeCyz"
   },
   "outputs": [],
   "source": [
    "#load dataset as a list of ints\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "#make all sequences of the same length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test =  pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZhMAgaNeCy5"
   },
   "outputs": [],
   "source": [
    "word_to_id= imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word={value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "print(id_to_word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dybtUgUReCy8"
   },
   "source": [
    "## Build Keras Embedding Layer Model\n",
    "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
    "\n",
    "* The embedding layer can be used at the start of a larger deep learning model. \n",
    "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
    "* Use the embedding layer to train our own word2vec models.\n",
    "\n",
    "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5OLM4eBeCy9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 1,535,194\n",
      "Trainable params: 1,535,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim,input_length = x_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxNDNhrseCzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 - 29s - loss: 0.5357 - accuracy: 0.7212\n",
      "Epoch 2/10\n",
      "25000/25000 - 29s - loss: 0.4218 - accuracy: 0.8056\n",
      "Epoch 3/10\n",
      "25000/25000 - 28s - loss: 0.3541 - accuracy: 0.8411\n",
      "Epoch 4/10\n",
      "25000/25000 - 30s - loss: 0.3024 - accuracy: 0.8693\n",
      "Epoch 5/10\n",
      "25000/25000 - 29s - loss: 0.2536 - accuracy: 0.8928\n",
      "Epoch 6/10\n",
      "25000/25000 - 28s - loss: 0.2130 - accuracy: 0.9114\n",
      "Epoch 7/10\n",
      "25000/25000 - 28s - loss: 0.1793 - accuracy: 0.9276\n",
      "Epoch 8/10\n",
      "25000/25000 - 28s - loss: 0.1479 - accuracy: 0.9418\n",
      "Epoch 9/10\n",
      "25000/25000 - 28s - loss: 0.1255 - accuracy: 0.9517\n",
      "Epoch 10/10\n",
      "25000/25000 - 28s - loss: 0.1097 - accuracy: 0.9583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fb1d838808>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(x_train, y_train, epochs = 10, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3CSVVPPeCzD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 - 4s - loss: 0.9483 - accuracy: 0.7367\n",
      "score: 0.95\n",
      "acc: 0.74\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(x_test, y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  75,   26,    2,  574,   19,    4, 1729,   23,    4,  268,   38,\n",
       "         95,  138,    4,  609,  191,   75,   28,  314, 1772])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igq8Qm8GeCzG"
   },
   "source": [
    "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "features = np.random.rand(1,maxlen)\n",
    "get_all_layer_outputs = K.function([model.layers[0].input],\n",
    "                                  [l.output for l in model.layers[1:]])\n",
    "\n",
    "layer_output = get_all_layer_outputs([features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.06500909, -0.00077358,  0.01465192, ...,  0.055968  ,\n",
      "         -0.05338809,  0.03880438],\n",
      "        [ 0.06500909, -0.00077358,  0.01465192, ...,  0.055968  ,\n",
      "         -0.05338809,  0.03880438],\n",
      "        [ 0.06500909, -0.00077358,  0.01465192, ...,  0.055968  ,\n",
      "         -0.05338809,  0.03880438],\n",
      "        ...,\n",
      "        [ 0.06500909, -0.00077358,  0.01465192, ...,  0.055968  ,\n",
      "         -0.05338809,  0.03880438],\n",
      "        [ 0.06500909, -0.00077358,  0.01465192, ...,  0.055968  ,\n",
      "         -0.05338809,  0.03880438],\n",
      "        [ 0.06500909, -0.00077358,  0.01465192, ...,  0.055968  ,\n",
      "         -0.05338809,  0.03880438]]], dtype=float32), array([[ 0.01151941, -0.02985353, -0.04485181, -0.02542052,  0.00611752,\n",
      "         0.00113654,  0.02062319, -0.04196103, -0.01966707, -0.1609039 ,\n",
      "        -0.08177948,  0.02433589, -0.06721227, -0.07411642,  0.00405345,\n",
      "        -0.08459633,  0.07390124,  0.0308803 ,  0.10148957, -0.0060621 ,\n",
      "        -0.04277287, -0.02356807, -0.3693281 ,  0.01572105,  0.03798866,\n",
      "         0.03911002, -0.01627671, -0.06907582,  0.11333007, -0.0191618 ,\n",
      "        -0.36400065,  0.0371408 , -0.02469567, -0.11712307,  0.01995936,\n",
      "         0.06320243,  0.08927558, -0.04455831, -0.0522458 ,  0.0699372 ,\n",
      "        -0.47052073,  0.18691467, -0.49096742,  0.1355403 , -0.07769316,\n",
      "         0.00736684, -0.06394517, -0.09857355,  0.04481904, -0.01652741,\n",
      "         0.03714429, -0.04231679, -0.00868226, -0.01293959,  0.3529789 ,\n",
      "        -0.01754186, -0.19849281,  0.40028992,  0.03195924, -0.05295824,\n",
      "        -0.01005277,  0.05515591, -0.01835397, -0.03740406, -0.39487934,\n",
      "         0.02356816, -0.03873887, -0.02846874, -0.0116945 , -0.11517268,\n",
      "         0.02502678,  0.03159145, -0.1583222 ,  0.09362238,  0.0008938 ,\n",
      "        -0.00250366, -0.26477394, -0.06417294, -0.04039754,  0.09920615,\n",
      "         0.01884928,  0.0285181 , -0.29457235, -0.15313323, -0.03697821,\n",
      "        -0.10924968,  0.5004853 ,  0.02283499,  0.02336114,  0.02034303,\n",
      "         0.08319718, -0.00823415, -0.00267875,  0.07572863,  0.09655558,\n",
      "        -0.05032375,  0.00196864, -0.00344841,  0.09267307, -0.01203818,\n",
      "         0.02575845,  0.11778159, -0.0074385 ,  0.01543452, -0.10959549,\n",
      "         0.2767112 ,  0.60637414, -0.06099574, -0.00105852, -0.05233427,\n",
      "        -0.00952314,  0.02560581, -0.19579376, -0.02034324,  0.35305333,\n",
      "        -0.02714755,  0.02617502, -0.06265995,  0.08778631, -0.04787277,\n",
      "         0.17772065,  0.42166516,  0.12407576, -0.00155469, -0.00875176,\n",
      "         0.14789368, -0.00931996, -0.00403259,  0.05758111, -0.47630253,\n",
      "        -0.0554383 , -0.08443311, -0.01839183, -0.00359425,  0.00845863,\n",
      "        -0.28002644,  0.3436509 , -0.37709317, -0.00809075,  0.03423999,\n",
      "        -0.15936525,  0.2758184 ,  0.13178846,  0.03936102,  0.11211742,\n",
      "        -0.04234652,  0.00541644, -0.17415383, -0.10051407, -0.03007279,\n",
      "        -0.03014297,  0.24413215,  0.02988935,  0.12099143,  0.04920003,\n",
      "        -0.01590864,  0.02541098,  0.00584055,  0.04099157,  0.07079107,\n",
      "        -0.03833162, -0.0146017 , -0.00224491, -0.17881602, -0.01314668,\n",
      "        -0.12936418, -0.02410373, -0.14267841,  0.01869077, -0.06926356,\n",
      "         0.07178298,  0.05434068,  0.07664452, -0.09637685,  0.00876824,\n",
      "        -0.25641757, -0.0985598 ,  0.00230327, -0.00430058,  0.16477905,\n",
      "         0.29714808, -0.3847994 ,  0.02533757, -0.00762298,  0.00398761,\n",
      "        -0.24465713, -0.13987637,  0.00658922,  0.13198887,  0.46757844,\n",
      "        -0.0154116 ,  0.21151443, -0.03033885, -0.29153463, -0.09328844,\n",
      "        -0.26291296]], dtype=float32), array([[0.99326915, 0.00673081]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "inp = model.input                                           # input \n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "functors = [K.function([inp], [out]) for out in outputs]    # evaluation functions\n",
    "\n",
    "# Testing\n",
    "test = x_test[3]\n",
    "count = 0\n",
    "for func in functors:\n",
    "  print('\\n')\n",
    "  print(\"Layer Name: \",layer_names[count])\n",
    "  print('\\n')\n",
    "  print(func([test]))\n",
    "  count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SeqNLP_Project1_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
